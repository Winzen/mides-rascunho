{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiM8n5Wi5PgS"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT-Xd5hN1rhu"
      },
      "source": [
        "# Meu drive pessoal - Variaveis Globais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPmnBa3NABTb"
      },
      "outputs": [],
      "source": [
        "id_grupo = 1\n",
        "path_drive_input = f\"/content/drive/MyDrive/DataBase/world_bank/rn/grupos_pagamentos/grupo_{id_grupo}/pagamentos_grupo_{id_grupo}\"\n",
        "path_drive_csv = f\"/content/drive/MyDrive/DataBase/world_bank/rn/grupos_pagamentos/grupo_{id_grupo}/links_coletados_pagamentos_grupo_{id_grupo}\"\n",
        "local_path_csv = f\"/content/links_coleta_grupo/links_coletados_pagamentos_grupo_{id_grupo}.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDO0GCnq1tYE"
      },
      "source": [
        "# Drive Compartilhado - Variaveis Globais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8xAsyuM12Db"
      },
      "outputs": [],
      "source": [
        "id_grupo = 1\n",
        "categoria = \"pagamentos\"\n",
        "path_drive_input = f\"/content/drive/MyDrive/DataBase/world_bank/ComprasPublicas_Brasil/input/RN/grupos_pagamento/grupo_{id_grupo}/{categoria}_grupo_{id_grupo}\"\n",
        "path_drive_csv = f\"/content/drive/MyDrive/DataBase/world_bank/ComprasPublicas_Brasil/input/RN/grupos_pagamento/grupo_{id_grupo}/links_coletados_{categoria}_grupo_{id_grupo}\"\n",
        "local_path_csv = f\"/content/links_coleta_grupo/links_coletados_pagamentos_grupo_{id_grupo}.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ptCC5xP2ssI"
      },
      "source": [
        "# Verificar IP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df7yG-uBRCoz",
        "outputId": "f684242d-2405-4a6a-c60f-64f9e92de003"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ip': '35.188.236.202', 'country': 'United States', 'cc': 'US'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "my_country = requests.get(\"https://api.myip.com/\")\n",
        "my_country.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQEPTfIo8LnX"
      },
      "source": [
        "# Importação e função"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knYMwEzg8QvQ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from urllib3 import PoolManager\n",
        "import os\n",
        "import csv\n",
        "import shutil\n",
        "import glob\n",
        "import concurrent.futures\n",
        "import datetime as dt\n",
        "import time\n",
        "import re\n",
        "import pandas as pd\n",
        "from time import sleep\n",
        "from more_itertools import batched\n",
        "import asyncio\n",
        "import aiohttp\n",
        "from aiohttp.client import ClientSession\n",
        "import nest_asyncio\n",
        "\n",
        "\n",
        "def csv_manager(path: str, rows: list, mode: str, fieldnames: list) -> None:\n",
        "\n",
        "      with open(path, mode, newline='', encoding='utf-8') as file:\n",
        "          # Create a CSV writer object\n",
        "          writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "\n",
        "          # Write the field names\n",
        "          if mode == 'w':\n",
        "              writer.writeheader()\n",
        "          writer.writerows(rows)\n",
        "\n",
        "def is_duplicate(path: str, row: dict) -> bool:\n",
        "  csv = csv_read(path)\n",
        "  for key, value in row.items():\n",
        "    if value == None:\n",
        "      row[key] = \"\"\n",
        "      continue\n",
        "    row[key] = str(value)\n",
        "  return row in csv\n",
        "\n",
        "def csv_read(path: str) -> list:\n",
        "  with open(path, 'r', encoding=\"utf-8\") as file:\n",
        "    existing_data = []\n",
        "    dict_reader = csv.DictReader(file)\n",
        "    for row in dict_reader:\n",
        "        existing_data.append(row)\n",
        "    return existing_data\n",
        "\n",
        "def empenhos_api(ano: int, identificador: int|str) -> str:\n",
        "\n",
        "  url = f\"https://apidadosabertos.tce.rn.gov.br/api/EmpenhosLiquidacoesPagamentosApi/Empenhos/Json/{ano}/{identificador}\"\n",
        "  # print(f\"Request para: {url}\")\n",
        "  # data = requests.get(url).json()\n",
        "\n",
        "  return url\n",
        "\n",
        "def liquidacoes_api(classificacaoInstitucional: str,\n",
        "                    dataEmpenho: str,\n",
        "                    numeroEmpenho: str,\n",
        "                    identificadorUnidade: int|str,\n",
        "                    formato: str = \"json\") -> str:\n",
        "\n",
        "  base_url = \"https://apidadosabertos.tce.rn.gov.br/api/EmpenhosLiquidacoesPagamentosApi/Liquidacoes/\"\n",
        "  parameters = f\"{formato}/{classificacaoInstitucional}/{dataEmpenho}/{numeroEmpenho}/{identificadorUnidade}\"\n",
        "  url = base_url + parameters\n",
        "\n",
        "  return url\n",
        "\n",
        "def pagamentos_api(classificacaoInstitucional: str|int,\n",
        "                    dataEmpenho: str,\n",
        "                    numeroEmpenho: str,\n",
        "                    identificadorUnidade: int,\n",
        "                    dataLiquidacao: str,\n",
        "                    numeroLiquidacao: str,\n",
        "                    formato: str = \"json\") -> str:\n",
        "\n",
        "  base_url = \"https://apidadosabertos.tce.rn.gov.br/api/EmpenhosLiquidacoesPagamentosApi/Pagamentos/\"\n",
        "\n",
        "  parameters = f\"{formato}/{classificacaoInstitucional}/{dataEmpenho}/{numeroEmpenho}/{dataLiquidacao}/{numeroLiquidacao}/{identificadorUnidade}\"\n",
        "  url = base_url + parameters\n",
        "\n",
        "  return url\n",
        "\n",
        "def coleted_list(path_input: str = \"/content/input\") -> list:\n",
        "  path_verify = f\"{path_input}/**/*/*/\"\n",
        "  csv_files_verify = glob.glob(path_verify, recursive=True)\n",
        "  ids = [folder.split(\"/\")[-2] for folder in csv_files_verify]\n",
        "  ids = list(dict.fromkeys(ids))\n",
        "  return ids\n",
        "\n",
        "\n",
        "def save_data(data: dict) -> None:\n",
        "\n",
        "  date = data[\"dataEmpenho\"].split(\"-\")\n",
        "  ano = date[0]\n",
        "  mes = str(date[1]).lstrip(\"0\")\n",
        "  columns = list(data.keys())\n",
        "  csv_file_name = f\"empenhos_{codigo_orgao}_{mes}_{ano}.csv\"\n",
        "  path_csv_folder = os.path.join(base_path, ano,\n",
        "              str(identificador_unidade))\n",
        "  path_csv = os.path.join(path_csv_folder, csv_file_name)\n",
        "\n",
        "  if not os.path.exists(path_csv_folder) or not os.path.exists(path_csv):\n",
        "    os.makedirs(path_csv_folder, exist_ok=True)\n",
        "    csv_manager(path_csv, [data], \"w\", columns)\n",
        "  else:\n",
        "    csv_manager(path_csv, [data], \"a\", columns)\n",
        "\n",
        "def get_year_json(link: str, year: int) -> tuple:\n",
        "  json = http.request('GET', link).json()\n",
        "  return (year, json)\n",
        "\n",
        "def make_list_chunks(values: list, chunk_size:int = 2000) -> list:\n",
        "  resquet_in_chunks = [chunk for chunk in batched(values, chunk_size)]\n",
        "  return resquet_in_chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTg1yuSe8InA"
      },
      "source": [
        "# Extrair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6to90Gj08LP3"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "def extrair(path, path_output) -> None:\n",
        "  # loading the temp.zip and creating a zip object\n",
        "  with ZipFile(path, 'r') as zObject:\n",
        "\n",
        "      # Extracting all the members of the zip\n",
        "      # into a specific location.\n",
        "      zObject.extractall(\n",
        "          path=path_output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "liqui = f\"/content/drive/MyDrive/DataBase/world_bank/ComprasPublicas_Brasil/input/RN/Liquidação/liquidacao_rn.zip\""
      ],
      "metadata": {
        "id": "fo4C_S5sAR4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extrair(path=liqui,\n",
        "        path_output=\"/content/liqui/\")"
      ],
      "metadata": {
        "id": "3HMlvQHKAijh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PaPn6j88Y0h"
      },
      "source": [
        "## Extrair CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8AKSSFj8WeR"
      },
      "outputs": [],
      "source": [
        "extrair(path=path_drive_csv + '.zip',\n",
        "        path_output=\"/content/links_coleta_grupo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeE-vEHaqy7n"
      },
      "outputs": [],
      "source": [
        "extrair(path=path_drive_input + '.zip',\n",
        "        path_output=\"/content/input\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCsWynZTLcJY"
      },
      "source": [
        "### Extrair Auto-Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHGcjwumLlcU"
      },
      "outputs": [],
      "source": [
        "extrair(path=path_drive_csv + \"_auto\" + '.zip',\n",
        "        path_output=\"/content/links_coleta_grupo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQXSGmDZIHaE"
      },
      "outputs": [],
      "source": [
        "extrair(path=path_drive_input + \"_auto\" + '.zip',\n",
        "        path_output=\"/content/input\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOYbLC779Au3"
      },
      "source": [
        "# Raspagem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsk9sU1z9CRH"
      },
      "outputs": [],
      "source": [
        "async def download_link(link: str, session: ClientSession) -> list:\n",
        "\n",
        "    # try:\n",
        "    async with session.get(link) as response:\n",
        "\n",
        "            result = await response.json()\n",
        "            return [link, result]\n",
        "\n",
        "    # except Exception as erro:\n",
        "    #     return erro\n",
        "\n",
        "async def download_all(links: list):\n",
        "    my_conn = aiohttp.TCPConnector(limit=20, ssl=False)\n",
        "    # timeout = aiohttp.ClientTimeout(total=20)\n",
        "    async with aiohttp.ClientSession(connector=my_conn) as session:\n",
        "        slots = []\n",
        "        for link in links:\n",
        "            slot = asyncio.ensure_future(download_link(link=link, session=session))\n",
        "            slots.append(slot)\n",
        "        await asyncio.gather(*slots, return_exceptions=True)# the await must be nest inside of the session\n",
        "    return slots\n",
        "\n",
        "\n",
        "def save_data_pagamento(data: dict, link: str) -> str|None:\n",
        "\n",
        "  try:\n",
        "\n",
        "    print(data)\n",
        "    id_unidade = link.split(\"/\")[-1]\n",
        "    data[\"identificador_unidade\"] = id_unidade\n",
        "    date = data[\"dataPagamento\"].split(\"-\")\n",
        "    ano = date[0]\n",
        "    mes = str(date[1]).lstrip(\"0\")\n",
        "    columns = list(data.keys())\n",
        "    csv_file_name = f\"pagamento_{mes}_{ano}.csv\"\n",
        "    path_csv_folder = os.path.join(base_path, ano,\n",
        "                str(id_unidade))\n",
        "    path_csv = os.path.join(path_csv_folder, csv_file_name)\n",
        "\n",
        "    if not os.path.exists(path_csv_folder) or not os.path.exists(path_csv):\n",
        "      os.makedirs(path_csv_folder, exist_ok=True)\n",
        "      csv_manager(path_csv, [data], \"w\", columns)\n",
        "    else:\n",
        "      csv_manager(path_csv, [data], \"a\", columns)\n",
        "\n",
        "    return link\n",
        "\n",
        "  except Exception as error:\n",
        "\n",
        "    print(f\"{'--' * 50}\\nErro: {error}\\n{data}\\n{'--' * 50}\")\n",
        "\n",
        "def get_csvs_from_id(id_unidade: str|int) -> list:\n",
        "  csv_orgao = f\"/content/input_empenho/**/{id_unidade}/*.csv\"\n",
        "  csvs = glob.glob(csv_orgao)\n",
        "  return csvs\n",
        "\n",
        "def get_json(link: str) -> list:\n",
        "\n",
        "  data_json = \"Vazio\"\n",
        "  response = \"\"\n",
        "\n",
        "  try:\n",
        "\n",
        "    response = http.request('GET', link)\n",
        "\n",
        "    if 400 <= response.status < 600:\n",
        "      raise Exception(f'Request failed with status code {response.status}')\n",
        "\n",
        "    data_json = response.json()\n",
        "\n",
        "  except Exception as error_get_json:\n",
        "    print(\"*-*\" * 30)\n",
        "    print(f\"Erro: {error_get_json}\\nFalha ao consegui o JSON\")\n",
        "    print(response.status_code)\n",
        "    print(response.text)\n",
        "    print(response.content)\n",
        "    print(\"*-*\" * 30)\n",
        "  finally:\n",
        "\n",
        "    return [link, data_json]\n",
        "\n",
        "def extract(links: list) -> list|dict:\n",
        "\n",
        "  coletados = []\n",
        "  tasks = asyncio.run(download_all(links))\n",
        "\n",
        "  datas = [slot.result() for slot in tasks if not slot.exception()]\n",
        "  datas = {link: json for link, json in datas if type(json) == list}\n",
        "  # datas = {key: data for key, data in datas.items() if type(data) == list}\n",
        "\n",
        "  for key, data in datas.items():\n",
        "    coletados = list(map(lambda data: save_data_pagamento(data, key), data))\n",
        "\n",
        "  print(coletados)\n",
        "  # return coletados\n",
        "  return list(datas.keys())\n",
        "\n",
        "def atualizar_df_coletados(df_coletados: pd.DataFrame, links: list) -> pd.DataFrame:\n",
        "  marks = df_coletados.link.isin(links)\n",
        "  df_coletados.loc[marks, [\"coletado\"]] = True\n",
        "  return df_coletados\n",
        "\n",
        "\n",
        "def send_folder_drive(path_drive: str, path_input: str) -> None:\n",
        "\n",
        "  if not len(os.listdir(path_input)) > 0:\n",
        "    raise Exception(\"Essa Pasta vazia\")\n",
        "    return None\n",
        "\n",
        "  shutil.make_archive(path_drive,\n",
        "                      'zip',\n",
        "                      path_input)\n",
        "\n",
        "\n",
        "def send_solo_drive() -> None:\n",
        "\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  send_folder_drive(path_drive_input + \"_auto\",\n",
        "                    \"/content/input/\")\n",
        "\n",
        "  send_folder_drive(path_drive_csv + \"_auto\",\n",
        "                    \"/content/links_coleta_grupo\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeOConAm9Jb_"
      },
      "outputs": [],
      "source": [
        "df_coletados = pd.read_csv(local_path_csv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBVJes_A9Lok",
        "outputId": "092a2d98-493b-4d54-c9d3-f3937f344f87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk: 1/401 Iniciado\n",
            "\n",
            "[]\n",
            "Foram Coletados: 500\n",
            "Duração da execução: 0.14\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "nest_asyncio.apply()\n",
        "base_path = os.path.join(os.getcwd(), \"input\")\n",
        "para_coletar = df_coletados[df_coletados.coletado == False].link.values\n",
        "links_chunk = make_list_chunks(para_coletar, chunk_size=500)\n",
        "\n",
        "for n, chunk in enumerate(links_chunk):\n",
        "\n",
        "  try:\n",
        "    start_time = time.time()\n",
        "    print(f\"Chunk: {n+1}/{len(links_chunk)} Iniciado\\n\")\n",
        "    links_coletados = extract(chunk)\n",
        "    print(f\"Foram Coletados: {len(links_coletados)}\")\n",
        "    df_coletados = atualizar_df_coletados(df_coletados, links_coletados)\n",
        "    print(f\"Duração da execução: {(time.time() - start_time) / 60:.2f}\\n{'-' * 30}\")\n",
        "    sleep(20)\n",
        "    break\n",
        "\n",
        "    # if (n + 1) % 10 == 0:\n",
        "    if len(links_coletados) == 0:\n",
        "      break\n",
        "    #   df_coletados.to_csv(local_path_csv, index=False)\n",
        "    #   send_solo_drive()\n",
        "\n",
        "  except Exception as erro:\n",
        "    print(f\"Erro: {erro}\\nFalha chunk {n+1}\")\n",
        "    df_coletados.to_csv(local_path_csv, index=False)\n",
        "    sleep(30)\n",
        "\n",
        "\n",
        "# send_solo_drive()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_x3fFGm7-1l1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i5GmwMGto51",
        "outputId": "9c4a00d4-6cef-440e-b8d4-98625f00919c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['https://apidadosabertos.tce.rn.gov.br/api/EmpenhosLiquidacoesPagamentosApi/Pagamentos/json/04100/2020-02-03/04100203001/2020-05-28/04100146/390',\n",
              "       False, nan, 1], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "df_coletados.loc[20000].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tjisyQCSdEK",
        "outputId": "bcf20bab-e976-408b-fda6-afe8aa7de5fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "201"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "para_coletar = df_coletados[df_coletados.coletado == False].link.values\n",
        "links_chunk = make_list_chunks(para_coletar, chunk_size=1000)\n",
        "len(links_chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5GM4P0zy8bk"
      },
      "source": [
        "# Verificar Dados corretos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMuYmmG3zEph"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def ler_csv(path: str = \"/content/verificar_files.csv\") -> list:\n",
        "\n",
        "  with open(path, \"r\") as txt_csv:\n",
        "\n",
        "    csv_rows = csv.DictReader(txt_csv)\n",
        "    rows = [row for row in csv_rows]\n",
        "    return rows\n",
        "\n",
        "\n",
        "def verificar_links(rows: list, link: str) -> int:\n",
        "  sum = 0\n",
        "  arg = link.split(\"/\")[-4::]\n",
        "  for row in rows:\n",
        "    if row.get(\"classificacaoInstitucional\") == str(arg[0]) and row.get(\"dataEmpenho\") == arg[1] and row.get(\"numeroEmpenho\") == arg[2] and row.get(\"identificador_unidade\") == str(arg[3]):\n",
        "      sum += 1\n",
        "\n",
        "  # mark = df_verifica[(df_verifica.classificacaoInstitucional == str(arg[0])) &\n",
        "  #  (df_verifica.dataEmpenho == arg[1]) &\n",
        "  #   (df_verifica.numeroEmpenho == arg[2]) &\n",
        "  #    (df_verifica.identificador_unidade == str(arg[3]))]\n",
        "  # return mark.shape[0]\n",
        "  return sum\n",
        "\n",
        "\n",
        "def criar_key(link: str) -> str:\n",
        "\n",
        "  arg = link.split(\"/\")[-4::]\n",
        "\n",
        "  key = \"\".join([str(arg[0]), arg[1],\n",
        "    arg[2], str(arg[3])])\n",
        "\n",
        "  return key\n",
        "\n",
        "\n",
        "def contar_encontros(row: pd.DataFrame, credibilidade: dict):\n",
        "\n",
        "  key = \"\".join([str(row.classificacaoInstitucional), row.dataEmpenho,\n",
        "    row.numeroEmpenho, str(row.identificador_unidade)])\n",
        "\n",
        "  try:\n",
        "    credibilidade[key][1] += 1\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  return credibilidade\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xiJYXKc6lE3",
        "outputId": "9887066f-3df0-40ab-d18a-e6047aee8130"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Foram coletados: 198264 links\n",
            "Numero de dados vinculados a um link: 78117\n",
            "Numero de dados possivelmente vazios e não vinculados: 120147\n",
            "\n"
          ]
        }
      ],
      "source": [
        "path_verify = f\"/content/input2/**/*.csv\"\n",
        "csv_files_verify = glob.glob(path_verify, recursive=True)\n",
        "\n",
        "df_verifica = pd.concat([pd.read_csv(csv_file, dtype=str) for csv_file in csv_files_verify])\n",
        "links_coletados = df_coletados[df_coletados.coletado == True].link.values\n",
        "credibilidade = {criar_key(link): [link, 0] for link in links_coletados}\n",
        "\n",
        "for row in df_verifica.itertuples():\n",
        "  credibilidade = contar_encontros(row, credibilidade)\n",
        "\n",
        "varios = {value[0]:value[1] for key, value in credibilidade.items() if value[1] == 0}\n",
        "encontrado = {value[0]:value[1] for key, value in credibilidade.items() if value[1] > 0}\n",
        "\n",
        "print(f\"Foram coletados: {len(links_coletados)} links\\n\" \\\n",
        "      f\"Numero de dados vinculados a um link: {len(encontrado)}\\n\" \\\n",
        "      f\"Numero de dados possivelmente vazios e não vinculados: {len(varios)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ7g3Ae_AMz1"
      },
      "source": [
        "## Verificar vazios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KDeRPlFq69El",
        "outputId": "e1678d97-bb0d-42ac-cdee-a1946b41f4ce"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://apidadosabertos.tce.rn.gov.br/api/EmpenhosLiquidacoesPagamentosApi/Liquidacoes/json/nan/2018-02-20/070016/454'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(varios.keys())[-10]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Liquidação ta certo"
      ],
      "metadata": {
        "id": "nKaeiw2XAqT1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LwACrnCkAtQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2IugeRyWweY"
      },
      "source": [
        "# Mandar para o drive Manualmente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45a__sGXWuRD",
        "outputId": "7e706e46-fdb9-4d1d-e373-06aeda828398"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "df_coletados.to_csv(local_path_csv, index=False)\n",
        "\n",
        "send_folder_drive(path_drive_input,\n",
        "                \"/content/input/\")\n",
        "\n",
        "send_folder_drive(path_drive_csv,\n",
        "                \"/content/links_coleta_grupo\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JQEPTfIo8LnX",
        "jCsWynZTLcJY",
        "wZ7g3Ae_AMz1"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}